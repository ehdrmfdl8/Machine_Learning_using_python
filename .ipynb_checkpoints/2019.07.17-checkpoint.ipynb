{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid의 문제점\n",
    "sigmoid의 한가지 문제점이 있다. 그중에 하나는 입력데이타 (z)값이 100이상 크면 출력값이 1로 수렴하거나 0으로\n",
    "수렴하게 된다. 따라서 normalization 을 하여 입력데이타를 scailing하여 입력데이타의 최대값으로 나누어서 0과 1사이의 값으로 만든다. 그리고 평균을 0으로 기준으로 잡아서 1과 -1사이에 있도록 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple classification class version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (10, 1) , t_data.shape =  (10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]).reshape(10,1)   \n",
    "t_data = np.array([0, 0, 0, 0,  0,  0,  1,  1,  1,  1]).reshape(10,1)\n",
    "\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class simple_classification:\n",
    "    def __init__(self,x_data,t_data,learning_rate,itertation_count):\n",
    "        if x_data.ndim == 1:\n",
    "            self.x_data = x_data.reshape(len(x_data),1)\n",
    "            self.t_data = x_data.reshape(len(t_data),1)\n",
    "        elif x_data.ndim == 2:\n",
    "            self.x_data = x_data\n",
    "            self.t_data = t_data\n",
    "        self.learning_rate = learning_rate\n",
    "        self.itertation_count = itertation_count\n",
    "        self.W = np.random.rand(self.x_data.shape[1],1)\n",
    "        self.b = np.random.rand(1)\n",
    "        print(\"simple_classification Object is created\")\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        self.z = z\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def loss_func(self):\n",
    "        delta = 1e-7 #log 무한대 발산 방지\n",
    "        \n",
    "        z = np.dot(self.x_data, self.W) + self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        #cross-entropy\n",
    "        return -np.sum(self.t_data*np.log(y + delta) + (1-self.t_data)*np.log((1-y)+delta) )\n",
    "    \n",
    "    def error_val(self):\n",
    "        delta = 1e-7 #log 무한대 발산 방지\n",
    "        \n",
    "        z = np.dot(self.x_data, self.W) + self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        #cross-entropy\n",
    "        return -np.sum(self.t_data*np.log(y + delta) + (1-self.t_data)*np.log((1-y)+delta) )\n",
    "    def predict(self,test_data):\n",
    "        z = np.dot(test_data, self.W) +self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        if y >= 0.5:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "        \n",
    "        return y,result\n",
    "    \n",
    "    def train(self):\n",
    "        f = lambda x : self.loss_func()\n",
    "        print(\"Initial error value = \", self.error_val(), \"Initial W = \", self.W, \"\\n\", \", b = \", self.b )\n",
    "        start_time = datetime.now()\n",
    "        for step in range(self.itertation_count):\n",
    "            self.W -= self.learning_rate * numerical_derivative(f, self.W)\n",
    "            self.b -= self.learning_rate * numerical_derivative(f, self.b)\n",
    "            \n",
    "            if(step % 5000 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val(), \"W = \", self.W, \", b = \",self.b)\n",
    "        end_time = datetime.now()\n",
    "        print(\"\")\n",
    "        print(\"Elapsed Time => \", end_time - start_time)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_classification Object is created\n",
      "Initial error value =  30.052109847100375 Initial W =  [[0.67314481]] \n",
      " , b =  [0.25455157]\n",
      "step =  0 error value =  13.177060858697386 W =  [[0.25978579]] , b =  [0.20345851]\n",
      "step =  5000 error value =  0.8445762090663494 W =  [[0.90030518]] , b =  [-11.55270509]\n",
      "step =  10000 error value =  0.6207235129314607 W =  [[1.15329992]] , b =  [-14.86841392]\n",
      "step =  15000 error value =  0.5093375697449064 W =  [[1.3332182]] , b =  [-17.21950182]\n",
      "step =  20000 error value =  0.43767615617936007 W =  [[1.47789528]] , b =  [-19.10743513]\n",
      "step =  25000 error value =  0.38606973621012 W =  [[1.60081606]] , b =  [-20.71014188]\n",
      "step =  30000 error value =  0.3464653269467817 W =  [[1.70857135]] , b =  [-22.11434989]\n",
      "step =  35000 error value =  0.31480137973872996 W =  [[1.80496199]] , b =  [-23.36997832]\n",
      "step =  40000 error value =  0.2887493913708191 W =  [[1.89241976]] , b =  [-24.50891933]\n",
      "step =  45000 error value =  0.2668531245954359 W =  [[1.97261566]] , b =  [-25.55306288]\n",
      "step =  50000 error value =  0.24814282985205988 W =  [[2.04675915]] , b =  [-26.51823816]\n",
      "step =  55000 error value =  0.23194140206875447 W =  [[2.11576072]] , b =  [-27.41635237]\n",
      "step =  60000 error value =  0.2177582855951174 W =  [[2.1803271]] , b =  [-28.25664238]\n",
      "step =  65000 error value =  0.20522741629707894 W =  [[2.24102048]] , b =  [-29.04645189]\n",
      "step =  70000 error value =  0.19406894201877703 W =  [[2.29829706]] , b =  [-29.79173764]\n",
      "step =  75000 error value =  0.18406456334895896 W =  [[2.35253325]] , b =  [-30.49741231]\n",
      "step =  80000 error value =  0.17504106508137005 W =  [[2.40404393]] , b =  [-31.16758462]\n",
      "step =  85000 error value =  0.1668589781418074 W =  [[2.45309573]] , b =  [-31.80573217]\n",
      "step =  90000 error value =  0.15940456832264746 W =  [[2.49991668]] , b =  [-32.41482889]\n",
      "step =  95000 error value =  0.1525840473766366 W =  [[2.54470363]] , b =  [-32.99744095]\n",
      "step =  100000 error value =  0.1463193074819197 W =  [[2.58762778]] , b =  [-33.55580028]\n",
      "step =  105000 error value =  0.140544723788262 W =  [[2.62883913]] , b =  [-34.09186176]\n",
      "step =  110000 error value =  0.13520472088782892 W =  [[2.66846992]] , b =  [-34.60734844]\n",
      "step =  115000 error value =  0.13025189541679635 W =  [[2.70663736]] , b =  [-35.10378762]\n",
      "step =  120000 error value =  0.12564554996563668 W =  [[2.7434459]] , b =  [-35.58254008]\n",
      "step =  125000 error value =  0.12135053554150306 W =  [[2.77898907]] , b =  [-36.04482384]\n",
      "step =  130000 error value =  0.1173363284890935 W =  [[2.81335092]] , b =  [-36.49173389]\n",
      "step =  135000 error value =  0.11357628765794663 W =  [[2.84660733]] , b =  [-36.92425839]\n",
      "step =  140000 error value =  0.11004705162099049 W =  [[2.87882703]] , b =  [-37.34329241]\n",
      "step =  145000 error value =  0.10672804577884942 W =  [[2.9100725]] , b =  [-37.74964937]\n",
      "step =  150000 error value =  0.10360107645726811 W =  [[2.94040068]] , b =  [-38.1440708]\n",
      "step =  155000 error value =  0.10064999444695859 W =  [[2.96986365]] , b =  [-38.5272347]\n",
      "step =  160000 error value =  0.09786041440176693 W =  [[2.99850915]] , b =  [-38.8997626]\n",
      "step =  165000 error value =  0.09521947948973807 W =  [[3.02638107]] , b =  [-39.26222579]\n",
      "step =  170000 error value =  0.09271566294887854 W =  [[3.05351983]] , b =  [-39.61515058]\n",
      "step =  175000 error value =  0.09033859992623329 W =  [[3.07996278]] , b =  [-39.95902296]\n",
      "step =  180000 error value =  0.08807894431236708 W =  [[3.10574447]] , b =  [-40.29429264]\n",
      "step =  185000 error value =  0.08592824631817564 W =  [[3.13089692]] , b =  [-40.62137658]\n",
      "step =  190000 error value =  0.08387884735406441 W =  [[3.15544991]] , b =  [-40.94066214]\n",
      "step =  195000 error value =  0.08192378941136101 W =  [[3.17943112]] , b =  [-41.25250979]\n",
      "step =  200000 error value =  0.08005673665471817 W =  [[3.20286639]] , b =  [-41.5572556]\n",
      "step =  205000 error value =  0.07827190734091476 W =  [[3.22577983]] , b =  [-41.85521336]\n",
      "step =  210000 error value =  0.07656401450584337 W =  [[3.24819399]] , b =  [-42.14667653]\n",
      "step =  215000 error value =  0.07492821412584635 W =  [[3.27012998]] , b =  [-42.43191996]\n",
      "step =  220000 error value =  0.0733600596740316 W =  [[3.29160763]] , b =  [-42.71120146]\n",
      "step =  225000 error value =  0.07185546216711657 W =  [[3.31264553]] , b =  [-42.98476314]\n",
      "step =  230000 error value =  0.07041065494256725 W =  [[3.3332612]] , b =  [-43.25283268]\n",
      "step =  235000 error value =  0.0690221625236292 W =  [[3.35347109]] , b =  [-43.51562448]\n",
      "step =  240000 error value =  0.06768677302830593 W =  [[3.37329076]] , b =  [-43.77334063]\n",
      "step =  245000 error value =  0.0664015136595143 W =  [[3.39273485]] , b =  [-44.02617187]\n",
      "step =  250000 error value =  0.06516362888175761 W =  [[3.41181722]] , b =  [-44.27429841]\n",
      "step =  255000 error value =  0.06397056094634979 W =  [[3.43055096]] , b =  [-44.5178907]\n",
      "step =  260000 error value =  0.06281993247522272 W =  [[3.4489485]] , b =  [-44.75711011]\n",
      "step =  265000 error value =  0.06170953085353957 W =  [[3.46702156]] , b =  [-44.99210958]\n",
      "step =  270000 error value =  0.0606372942153674 W =  [[3.48478132]] , b =  [-45.22303418]\n",
      "step =  275000 error value =  0.05960129883594616 W =  [[3.50223836]] , b =  [-45.45002166]\n",
      "step =  280000 error value =  0.0585997477681126 W =  [[3.51940273]] , b =  [-45.6732029]\n",
      "step =  285000 error value =  0.05763096058215992 W =  [[3.53628402]] , b =  [-45.8927024]\n",
      "step =  290000 error value =  0.05669336408587525 W =  [[3.55289132]] , b =  [-46.10863867]\n",
      "step =  295000 error value =  0.05578548391723736 W =  [[3.56923332]] , b =  [-46.32112461]\n",
      "step =  300000 error value =  0.05490593691553188 W =  [[3.58531829]] , b =  [-46.53026785]\n",
      "step =  305000 error value =  0.05405342418788972 W =  [[3.60115413]] , b =  [-46.73617107]\n",
      "step =  310000 error value =  0.05322672479860856 W =  [[3.61674837]] , b =  [-46.93893233]\n",
      "step =  315000 error value =  0.052424690016788095 W =  [[3.63210822]] , b =  [-47.1386453]\n",
      "step =  320000 error value =  0.05164623806553946 W =  [[3.64724055]] , b =  [-47.33539954]\n",
      "step =  325000 error value =  0.05089034932248434 W =  [[3.66215197]] , b =  [-47.52928075]\n",
      "step =  330000 error value =  0.05015606192693676 W =  [[3.67684876]] , b =  [-47.72037096]\n",
      "step =  335000 error value =  0.0494424677541245 W =  [[3.69133699]] , b =  [-47.90874875]\n",
      "step =  340000 error value =  0.04874870872114527 W =  [[3.70562243]] , b =  [-48.09448945]\n",
      "step =  345000 error value =  0.04807397339329329 W =  [[3.71971064]] , b =  [-48.27766528]\n",
      "step =  350000 error value =  0.04741749386251589 W =  [[3.73360695]] , b =  [-48.45834559]\n",
      "step =  355000 error value =  0.046778542873019684 W =  [[3.74731648]] , b =  [-48.63659691]\n",
      "step =  360000 error value =  0.04615643117142711 W =  [[3.76084414]] , b =  [-48.81248321]\n",
      "step =  365000 error value =  0.04555050506131072 W =  [[3.77419466]] , b =  [-48.98606593]\n",
      "step =  370000 error value =  0.04496014414395404 W =  [[3.78737258]] , b =  [-49.15740418]\n",
      "step =  375000 error value =  0.04438475922901148 W =  [[3.80038227]] , b =  [-49.32655482]\n",
      "step =  380000 error value =  0.043823790400368115 W =  [[3.81322795]] , b =  [-49.49357261]\n",
      "step =  385000 error value =  0.04327670522390717 W =  [[3.82591367]] , b =  [-49.65851024]\n",
      "step =  390000 error value =  0.04274299708516175 W =  [[3.83844333]] , b =  [-49.82141853]\n",
      "step =  395000 error value =  0.042222183646089984 W =  [[3.8508207]] , b =  [-49.98234644]\n",
      "step =  400000 error value =  0.04171380541105542 W =  [[3.8630494]] , b =  [-50.14134118]\n",
      "\n",
      "Elapsed Time =>  0:00:35.093981\n"
     ]
    }
   ],
   "source": [
    "obj = simple_classification(x_data,t_data,1e-2,400001)\n",
    "obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.70013857e-16] 0\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([3.7])\n",
    "(real_val, logical_val) = obj.predict(test_data)\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.] 1\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([31.09])\n",
    "\n",
    "(real_val, logical_val) = obj.predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 2 함수버젼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (9, 2) , t_data.shape =  (9, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "x_data = np.array([[2, 4], [4, 11], [6, 6], [8, 5], [10, 7], [12, 16], [14, 8], [16, 3], [18, 7]]).reshape(-1,2)  \n",
    "t_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1]).reshape(-1,1)\n",
    "\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.98796057]\n",
      " [0.66435537]] , W.shape =  (2, 1) , b =  [[0.94732909]] , b.shape =  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(xdata.shape[1],1).reshape(-1,1)\n",
    "b = np.random.rand(1).reshape(-1,1)\n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "def loss_func(x, t):\n",
    "    \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 test_data : numpy type\n",
    "def predict(test_data):\n",
    "    \n",
    "    z = np.dot(test_data.reshape(1,2), W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y >= 0.5:\n",
    "        result = 1  # True\n",
    "    else:\n",
    "        result = 0  # False\n",
    "    \n",
    "    return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  40.78135135528511 Initial W =  [[0.98796057]\n",
      " [0.66435537]] \n",
      " , b =  [[0.94732909]]\n",
      "step =  0 error value =  30.063984905482606 W =  [[0.79065242]\n",
      " [0.40792761]] , b =  [[0.90750844]]\n",
      "step =  5000 error value =  0.6071423712554307 W =  [[1.03331389]\n",
      " [0.15942882]] , b =  [[-10.16551728]]\n",
      "step =  10000 error value =  0.40530810145974927 W =  [[1.27915845]\n",
      " [0.31164694]] , b =  [[-13.30640032]]\n",
      "step =  15000 error value =  0.3075722959114161 W =  [[1.43934389]\n",
      " [0.43758295]] , b =  [[-15.5034969]]\n",
      "step =  20000 error value =  0.24773333892738206 W =  [[1.56462848]\n",
      " [0.53674506]] , b =  [[-17.2244979]]\n",
      "step =  25000 error value =  0.20719138824881694 W =  [[1.66863141]\n",
      " [0.61718719]] , b =  [[-18.64170633]]\n",
      "step =  30000 error value =  0.17792508746022964 W =  [[1.75764654]\n",
      " [0.68460192]] , b =  [[-19.8460893]]\n",
      "step =  35000 error value =  0.1558247986964327 W =  [[1.83543389]\n",
      " [0.742548  ]] , b =  [[-20.89282252]]\n",
      "step =  40000 error value =  0.13855878354973433 W =  [[1.90448349]\n",
      " [0.79332768]] , b =  [[-21.81808877]]\n",
      "step =  45000 error value =  0.12470533082531582 W =  [[1.96654047]\n",
      " [0.83850267]] , b =  [[-22.646928]]\n",
      "step =  50000 error value =  0.1133487599120712 W =  [[2.02287696]\n",
      " [0.87917753]] , b =  [[-23.39738821]]\n",
      "step =  55000 error value =  0.10387306664537717 W =  [[2.07444879]\n",
      " [0.91616119]] , b =  [[-24.08290392]]\n",
      "step =  60000 error value =  0.09584884607334339 W =  [[2.1219921 ]\n",
      " [0.95006336]] , b =  [[-24.71374216]]\n",
      "step =  65000 error value =  0.08896776015643945 W =  [[2.16608562]\n",
      " [0.98135475]] , b =  [[-25.29792345]]\n",
      "step =  70000 error value =  0.08300277107530264 W =  [[2.20719243]\n",
      " [1.01040628]] , b =  [[-25.84183108]]\n",
      "step =  75000 error value =  0.07778305919731981 W =  [[2.24568868]\n",
      " [1.03751531]] , b =  [[-26.35062721]]\n",
      "step =  80000 error value =  0.07317767280718057 W =  [[2.28188403]\n",
      " [1.06292387]] , b =  [[-26.82854494]]\n",
      "step =  85000 error value =  0.06908456167806115 W =  [[2.31603638]\n",
      " [1.08683156]] , b =  [[-27.27909805]]\n",
      "step =  90000 error value =  0.06542303621375921 W =  [[2.34836275]\n",
      " [1.10940487]] , b =  [[-27.7052349]]\n",
      "step =  95000 error value =  0.06212846701144344 W =  [[2.37904746]\n",
      " [1.13078415]] , b =  [[-28.10945324]]\n",
      "step =  100000 error value =  0.059148485807335394 W =  [[2.40824837]\n",
      " [1.15108876]] , b =  [[-28.4938874]]\n",
      "\n",
      "Elapsed Time =>  0:00:11.823405\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2  # 발산하는 경우, 1e-3 ~ 1e-6 등으로 바꾸어서 실행\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)  # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(100001):  \n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 5000 == 0):\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \",b )\n",
    "        \n",
    "end_time = datetime.now()\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15438257]] 0\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([3,17])\n",
    "\n",
    "(real_val, logical_val) = predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[[0.00071398]] 0\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([5,8])\n",
    "print(test_data.shape)\n",
    "(real_val, logical_val) = predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999641]] 1\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([7,21])\n",
    "\n",
    "(real_val, logical_val) = predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59991071]] 1\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([12,0])\n",
    "\n",
    "(real_val, logical_val) = predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 같은예제 class 버젼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (9, 2) , t_data.shape =  (9, 1)\n"
     ]
    }
   ],
   "source": [
    "# 입력데이터 / 정답데이터 세팅\n",
    "\n",
    "x_data = np.array( [ [2, 4], [4, 11], [6, 6], [8, 5], [10, 7], [12, 16], [14, 8], [16, 3], [18, 7] ] )\n",
    "t_data = np.array( [0, 0, 0, 0, 1, 1, 1, 1, 1] ).reshape(9, 1)\n",
    "\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class simple_classification:\n",
    "    def __init__(self,x_data,t_data,learning_rate,itertation_count):\n",
    "        if x_data.ndim == 1:\n",
    "            self.x_data = x_data.reshape(len(x_data),1)\n",
    "            self.t_data = x_data.reshape(len(t_data),1)\n",
    "        elif x_data.ndim == 2:\n",
    "            self.x_data = x_data\n",
    "            self.t_data = t_data\n",
    "        self.learning_rate = learning_rate\n",
    "        self.itertation_count = itertation_count\n",
    "        self.W = np.random.rand(self.x_data.shape[1],1)\n",
    "        self.b = np.random.rand(1)\n",
    "        print(\"simple_classification Object is created\")\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        self.z = z\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def loss_func(self):\n",
    "        delta = 1e-7 #log 무한대 발산 방지\n",
    "        \n",
    "        z = np.dot(self.x_data, self.W) + self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        #cross-entropy\n",
    "        return -np.sum(self.t_data*np.log(y + delta) + (1-self.t_data)*np.log((1-y)+delta) )\n",
    "    \n",
    "    def error_val(self):\n",
    "        delta = 1e-7 #log 무한대 발산 방지\n",
    "        \n",
    "        z = np.dot(self.x_data, self.W) + self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        #cross-entropy\n",
    "        return -np.sum(self.t_data*np.log(y + delta) + (1-self.t_data)*np.log((1-y)+delta) )\n",
    "    def predict(self,test_data):\n",
    "        z = np.dot(test_data, self.W) +self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        if y >= 0.5:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "        \n",
    "        return y,result\n",
    "    \n",
    "    def train(self):\n",
    "        f = lambda x : self.loss_func()\n",
    "        print(\"Initial error value = \", self.error_val(), \"Initial W = \", self.W, \"\\n\", \", b = \", self.b )\n",
    "        start_time = datetime.now()\n",
    "        for step in range(self.itertation_count):\n",
    "            self.W -= self.learning_rate * numerical_derivative(f, self.W)\n",
    "            self.b -= self.learning_rate * numerical_derivative(f, self.b)\n",
    "            \n",
    "            if(step % 5000 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val(), \"W = \", self.W, \", b = \",self.b)\n",
    "        end_time = datetime.now()\n",
    "        print(\"\")\n",
    "        print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_classification Object is created\n",
      "Initial error value =  19.635583465288835 Initial W =  [[0.4028083 ]\n",
      " [0.38530978]] \n",
      " , b =  [0.37103445]\n",
      "step =  0 error value =  9.359179445966642 W =  [[0.20497506]\n",
      " [0.12865747]] , b =  [0.33618511]\n",
      "step =  5000 error value =  0.6042370342395623 W =  [[1.03631356]\n",
      " [0.16084248]] , b =  [-10.20139745]\n",
      "step =  10000 error value =  0.404158735518676 W =  [[1.28082433]\n",
      " [0.31290404]] , b =  [-13.32895527]\n",
      "step =  15000 error value =  0.30691411150809084 W =  [[1.44058228]\n",
      " [0.4385711 ]] , b =  [-15.52055938]\n",
      "step =  20000 error value =  0.24730385538395264 W =  [[1.56563579]\n",
      " [0.53753311]] , b =  [-17.23827808]\n",
      "step =  25000 error value =  0.2068892061342476 W =  [[1.66948293]\n",
      " [0.61783814]] , b =  [-18.65326359]\n",
      "step =  30000 error value =  0.17770118176539879 W =  [[1.75838399]\n",
      " [0.6851553 ]] , b =  [-19.85603664]\n",
      "step =  35000 error value =  0.1556524158910988 W =  [[1.83608398]\n",
      " [0.74302882]] , b =  [-20.90154993]\n",
      "step =  40000 error value =  0.13842207869977058 W =  [[1.9050645 ]\n",
      " [0.79375257]] , b =  [-21.82586021]\n",
      "step =  45000 error value =  0.12459432926350192 W =  [[1.96706552]\n",
      " [0.83888316]] , b =  [-22.65393045]\n",
      "step =  50000 error value =  0.11325687471955755 W =  [[2.02335576]\n",
      " [0.87952196]] , b =  [-23.40375893]\n",
      "step =  55000 error value =  0.10379577635834235 W =  [[2.07488876]\n",
      " [0.91647575]] , b =  [-24.08874658]\n",
      "step =  60000 error value =  0.09578294595107008 W =  [[2.12239901]\n",
      " [0.95035277]] , b =  [-24.71913698]\n",
      "step =  65000 error value =  0.08891091658027439 W =  [[2.16646406]\n",
      " [0.98162272]] , b =  [-25.30293373]\n",
      "step =  70000 error value =  0.08295324511306028 W =  [[2.20754608]\n",
      " [1.01065574]] , b =  [-25.84650764]\n",
      "step =  75000 error value =  0.07773952896280362 W =  [[2.24602057]\n",
      " [1.03774864]] , b =  [-26.35501147]\n",
      "step =  80000 error value =  0.07313911579278026 W =  [[2.28219667]\n",
      " [1.06314301]] , b =  [-26.83267107]\n",
      "step =  85000 error value =  0.0690501747289496 W =  [[2.31633186]\n",
      " [1.08703813]] , b =  [-27.28299461]\n",
      "step =  90000 error value =  0.06539217982540657 W =  [[2.34864285]\n",
      " [1.10960023]] , b =  [-27.70892595]\n",
      "step =  95000 error value =  0.06210062575279901 W =  [[2.37931369]\n",
      " [1.13096945]] , b =  [-28.11295927]\n",
      "step =  100000 error value =  0.059123239681247323 W =  [[2.40850204]\n",
      " [1.15126498]] , b =  [-28.49722598]\n",
      "\n",
      "Elapsed Time =>  0:00:11.958017\n"
     ]
    }
   ],
   "source": [
    "obj1 = simple_classification(x_data, t_data, 1e-2, 100001)\n",
    "\n",
    "obj1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15443716] 0\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([3, 17]) # (예습, 복습) = (3, 17) => Fail (0)\n",
    "\n",
    "(real_val, logical_val) = obj1.predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00071351] 0\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([5, 8]) # (예습, 복습) = (5, 8) => Fail (0)\n",
    "\n",
    "(real_val, logical_val) = obj1.predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999642] 1\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([7, 21]) # (예습, 복습) = (7, 21) => Pass (1)\n",
    "\n",
    "(real_val, logical_val) = obj1.predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59984] 1\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([12, 0])  # (예습, 복습) = (12, 0) => Pass (1)\n",
    "\n",
    "(real_val, logical_val) = obj1.predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (4, 2) , t_data.shape =  (4, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "t_data = np.array([0,0,0,1]).reshape(4,1)\n",
    "test_data = np.array([ [0,0], [0,1], [1,0], [1,1] ])\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class simple_classification:\n",
    "    def __init__(self,x_data,t_data,learning_rate,itertation_count):\n",
    "        if x_data.ndim == 1:\n",
    "            self.x_data = x_data.reshape(len(x_data),1)\n",
    "            self.t_data = x_data.reshape(len(t_data),1)\n",
    "        elif x_data.ndim == 2:\n",
    "            self.x_data = x_data\n",
    "            self.t_data = t_data\n",
    "        self.learning_rate = learning_rate\n",
    "        self.itertation_count = itertation_count\n",
    "        self.W = np.random.rand(self.x_data.shape[1],1)\n",
    "        self.b = np.random.rand(1)\n",
    "        print(\"simple_classification Object is created \")\n",
    "        print(\"W = \",self.W,\"W.shape\",self.W.shape)\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        self.z = z\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def loss_func(self):\n",
    "        delta = 1e-7 #log 무한대 발산 방지\n",
    "        \n",
    "        z = np.dot(self.x_data, self.W) + self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        #cross-entropy\n",
    "        return -np.sum(self.t_data*np.log(y + delta) + (1-self.t_data)*np.log((1-y)+delta) )\n",
    "    \n",
    "    def error_val(self):\n",
    "        delta = 1e-7 #log 무한대 발산 방지\n",
    "        \n",
    "        z = np.dot(self.x_data, self.W) + self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        #cross-entropy\n",
    "        return -np.sum(self.t_data*np.log(y + delta) + (1-self.t_data)*np.log((1-y)+delta) )\n",
    "    def predict(self,test_data):\n",
    "        z = np.dot(test_data, self.W) +self.b\n",
    "        y = self.sigmoid(z)\n",
    "        \n",
    "        if y >= 0.5:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "        \n",
    "        return y,result\n",
    "    \n",
    "    def train(self):\n",
    "        f = lambda x : self.loss_func()\n",
    "        print(\"Initial error value = \", self.error_val(), \"Initial W = \", self.W, \"\\n\", \", b = \", self.b )\n",
    "        start_time = datetime.now()\n",
    "        for step in range(self.itertation_count):\n",
    "            self.W -= self.learning_rate * numerical_derivative(f, self.W)\n",
    "            self.b -= self.learning_rate * numerical_derivative(f, self.b)\n",
    "            \n",
    "            if(step % 5000 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val(), \"W = \", self.W, \", b = \",self.b)\n",
    "        end_time = datetime.now()\n",
    "        print(\"\")\n",
    "        print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_classification Object is created \n",
      "W =  [[0.2665108 ]\n",
      " [0.52847045]] W.shape (2, 1)\n",
      "Initial error value =  3.698268626157702 Initial W =  [[0.2665108 ]\n",
      " [0.52847045]] \n",
      " , b =  [0.49991148]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-37a31d7d70df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mobj2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mobj2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-68614db5c363>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initial error value = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Initial W = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\", b = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitertation_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnumerical_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "obj2 = simple_classification(x_data,t_data,1e-2,100001)\n",
    "obj2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5] 1\n",
      "[0.5] 0\n",
      "[0.5] 1\n",
      "[0.5] 1\n"
     ]
    }
   ],
   "source": [
    "# XOR 문제의 문제점\n",
    "for input_data in test_data:\n",
    "\n",
    "\n",
    "    (real_val, logical_val) = obj2.predict(input_data)\n",
    "\n",
    "    print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
