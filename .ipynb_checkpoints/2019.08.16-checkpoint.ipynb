{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneration:\n",
    "    \n",
    "    def __init__(self, name, file_path, seperation_rate, is_normalized = False):\n",
    "        self.name = name\n",
    "        self.file_path = file_path\n",
    "        self.seperation_rate = seperation_rate\n",
    "        self.is_normalized = is_normalized\n",
    "        print(\"DataGeneration object is created !!\")\n",
    "        \n",
    "    def data_normalized_using_min_max(self, loaded_data):\n",
    "        transpose_loaded_data = loaded_data.T\n",
    "        \n",
    "        print(\"transpose_loaded_data.shape = \", transpose_loaded_data)\n",
    "        \n",
    "        transpose_normalized_data_list = []\n",
    "        \n",
    "        for index in range(len(transpose_loaded_data)):\n",
    "            max_value = np.max(transpose_loaded_data[index, :])\n",
    "            min_value = np.min(transpose_loaded_data[index, :])\n",
    "            transpose_normalized_data_list.append((transpose_loaded_data[index, :] - min_value)/(max_value - min_value))\n",
    "        \n",
    "        transpose_normalized_data = np.array(transpose_normalized_data_list)\n",
    "        print(\"transpose_normalized_data.shape = \",transpose_normalized_data.shape)\n",
    "        \n",
    "        normalized_data = transpose_normalized_data.T\n",
    "        print(\"normalized_data.shape = \", normalized_data.shape)\n",
    "        \n",
    "        data_save_path = './Normalized_' + self.name + '_data.csv'\n",
    "        \n",
    "        np.savetxt(data_save_path, normalized_data, delimiter= ',')\n",
    "        \n",
    "        return normalized_data\n",
    "    \n",
    "    def generate(self):\n",
    "        loaded_data = np.loadtxt(self.file_path, delimiter= ',', dtype = np.float32)\n",
    "        print(\"loaded_data.shape = \", loaded_data.shape)\n",
    "        \n",
    "        if self.is_normalized == True:\n",
    "            loaded_data = self.data_normalized_using_min_max(loaded_data)\n",
    "        \n",
    "        training_data_list = []\n",
    "        test_data_list = []\n",
    "        \n",
    "        total_data_num = len(loaded_data)\n",
    "        test_data_num = int(len(loaded_data) * self.seperation_rate)\n",
    "        \n",
    "        total_data_index_list = [index for index in range(total_data_num)]\n",
    "        \n",
    "        random.shuffle(total_data_index_list)\n",
    "        \n",
    "        ########################### test data와 training data 나누기 ######################################\n",
    "        \n",
    "        test_data_index_list = total_data_index_list[ 0: test_data_num]\n",
    "        \n",
    "        print(\"length of test_data_index_list = \", len(test_data_index_list))\n",
    "        \n",
    "        #training data 를 위한 인덱스는 total_data_index_list 에서 test data인덱스를 제외한 나머지 부분\n",
    "        training_data_index_list = total_data_index_list[ test_data_num : ]\n",
    "        \n",
    "        print(\"length of training_data_index_list = \", len(training_data_index_list))\n",
    "        \n",
    "        # training data 구성\n",
    "        for training_data_index in training_data_index_list:\n",
    "            training_data_list.append(loaded_data[training_data_index])\n",
    "        \n",
    "        # test data 구성\n",
    "        for test_data_index in test_data_index_list:\n",
    "            test_data_list.append(loaded_data[test_data_index])\n",
    "            \n",
    "        #generate training data from training_data list using np.array(___)\n",
    "        training_data = np.array(training_data_list)\n",
    "        \n",
    "        #generate test data from test_data list using np.array(___)\n",
    "        test_data = np.array(test_data_list)\n",
    "        \n",
    "        #verification shape\n",
    "        print(\"training_data.shape = \", training_data.shape)\n",
    "        print(\"test_data.shape = \", test_data.shape)\n",
    "        \n",
    "        #save training & test data(.csv)\n",
    "        training_data_save_path = './random_' + self.name + '_training_data.csv'\n",
    "        test_data_save_path = './random_' + self.name + '_test_data.csv'\n",
    "        \n",
    "        np.savetxt(training_data_save_path, training_data, delimiter= ',')\n",
    "        np.savetxt(test_data_save_path, test_data, delimiter=',')\n",
    "        \n",
    "        return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperation_rate = 0.3\n",
    "data_obj = DataGeneration('ThoracicSurgery', './ThoracicSurgery.csv', seperation_rate, True)\n",
    "\n",
    "(training_data, test_data) = data_obj.generate()\n",
    "\n",
    "print(\"training_data.shape = \", training_data.shape)\n",
    "print(\"test_data.shape = \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
