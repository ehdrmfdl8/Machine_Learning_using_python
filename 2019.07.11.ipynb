{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.71120761]] , W.shape = (1, 1) ,b=  [0.55194222] ,b.shape = (1,)\n",
      "initial error value = 31.565190545332662 Initial W =  [[0.71120761]] \n",
      " , b =  [0.55194222]\n",
      "step = 0 error value =  18.65395644703595 W =  [[1.0816254]] , b =  [0.63600585]\n",
      "step = 400 error value =  0.01404534786781012 W =  [[2.07695963]] , b =  [1.72221931]\n",
      "step = 800 error value =  0.0008961762296808918 W =  [[2.01943987]] , b =  [1.92983306]\n",
      "step = 1200 error value =  5.718134162315837e-05 W =  [[2.00491048]] , b =  [1.98227595]\n",
      "step = 1600 error value =  3.6485076500966912e-06 W =  [[2.00124038]] , b =  [1.99552293]\n",
      "step = 2000 error value =  2.3279635795439923e-07 W =  [[2.00031332]] , b =  [1.9988691]\n",
      "step = 2400 error value =  1.485378392320503e-08 W =  [[2.00007914]] , b =  [1.99971434]\n",
      "step = 2800 error value =  9.477592294582006e-10 W =  [[2.00001999]] , b =  [1.99992784]\n",
      "step = 3200 error value =  6.04726419529428e-11 W =  [[2.00000505]] , b =  [1.99998177]\n",
      "step = 3600 error value =  3.858512068379724e-12 W =  [[2.00000128]] , b =  [1.9999954]\n",
      "step = 4000 error value =  2.4619588083720653e-13 W =  [[2.00000032]] , b =  [1.99999884]\n",
      "step = 4400 error value =  1.57087527792561e-14 W =  [[2.00000008]] , b =  [1.99999971]\n",
      "step = 4800 error value =  1.0023112621701332e-15 W =  [[2.00000002]] , b =  [1.99999993]\n",
      "step = 5200 error value =  6.39533823771875e-17 W =  [[2.00000001]] , b =  [1.99999998]\n",
      "step = 5600 error value =  4.080601337137762e-18 W =  [[2.]] , b =  [2.]\n",
      "step = 6000 error value =  2.6036601799598947e-19 W =  [[2.]] , b =  [2.]\n",
      "step = 6400 error value =  1.6612700700972723e-20 W =  [[2.]] , b =  [2.]\n",
      "step = 6800 error value =  1.0599749482689336e-21 W =  [[2.]] , b =  [2.]\n",
      "step = 7200 error value =  6.76287344356285e-23 W =  [[2.]] , b =  [2.]\n",
      "step = 7600 error value =  4.3195567051474204e-24 W =  [[2.]] , b =  [2.]\n",
      "step = 8000 error value =  2.759945050607872e-25 W =  [[2.]] , b =  [2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([88.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x,flags=['multi_index'],op_flags = ['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "t_data = np.array([4,6,8,10,12]).reshape(5,1)\n",
    "\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W =\", W,\", W.shape =\", W.shape, \",b= \", b,\",b.shape =\", b.shape)\n",
    "\n",
    "def loss_func(x,t):\n",
    "    y = np.dot(x,W) + b\n",
    "    return(np.sum((t-y)**2)) / (len(x)) #오차를 양수로 만들기 위해,오차의 가중치를 주기 위해 제곱을 한다.\n",
    "def error_val(x,t):\n",
    "    y = np.dot(x,W) + b\n",
    "    return(np.sum((t-y)**2)) / (len(x))\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    return y\n",
    "\n",
    "learning_rate = 1e-2\n",
    "f = lambda x : loss_func(x_data,t_data)\n",
    "print(\"initial error value =\", error_val(x_data,t_data),\"Initial W = \", W, \"\\n\",\", b = \", b)\n",
    "for step in range(8001):\n",
    "    W -= learning_rate * numerical_derivative(f,W)\n",
    "    b -= learning_rate * numerical_derivative(f,b)\n",
    "    \n",
    "    if(step % 400 == 0):\n",
    "        print(\"step =\", step, \"error value = \", error_val(x_data,t_data), \"W = \", W,\", b = \",b)\n",
    "test_score = np.array([43])\n",
    "predict(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
